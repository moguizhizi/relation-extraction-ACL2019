{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import keras\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding,\\\n",
    "Dropout, Activation,GRU,Bidirectional,Subtract, Permute, TimeDistributed, Reshape\n",
    "from keras.layers import Conv1D,Conv2D,MaxPooling2D,GlobalAveragePooling1D,GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, SpatialDropout1D,Layer\n",
    "from keras.layers.merge import concatenate, Concatenate, Average, Dot, Maximum, Multiply, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import *\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.special import erfinv\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_all_english.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "data['between_docs'] = data['between_docs'].map(eval)\n",
    "data['docs_'] = data['docs_'].map(eval)\n",
    "tokenizer_api = Tokenizer()\n",
    "tokenizer_api.fit_on_texts(data['docs_'])\n",
    "\n",
    "data_char_seq = tokenizer_api.texts_to_sequences(data['between_docs'])\n",
    "data['char_seq'] = pad_sequences(data_char_seq, maxlen = 16, padding='post').tolist()\n",
    "\n",
    "LB = LabelEncoder()\n",
    "LB.fit(list(data['entity_type_1']) + list(data['entity_type_2']))\n",
    "print(len(LB.classes_))\n",
    "\n",
    "data['entity_type_1_LB'] = LB.transform(data['entity_type_1'])\n",
    "data['entity_type_2_LB'] = LB.transform(data['entity_type_2'])\n",
    "\n",
    "data['between_tag'] = data['between_tag'].apply(eval)\n",
    "\n",
    "data['tag_all_seq'] = pad_sequences(data['between_tag'], maxlen = 16, padding='post').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['relation'] = data['relation'].fillna('Other')\n",
    "label = list(data['relation'].unique())\n",
    "label.remove('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2 = data[(data['relation'] != 'Other')|((data['relation'] == 'Other')&(data['entity_start_1'] < data['entity_start_2']))]\n",
    "data = d2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.index = range(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['relation'].values\n",
    "\n",
    "LB_y = LabelEncoder()\n",
    "LB_y.fit(y)\n",
    "y = LB_y.transform(y)\n",
    "\n",
    "y = to_categorical(y, np.max(y) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_start_1_in = []\n",
    "entity_start_2_in = []\n",
    "entity_end_1_in = []\n",
    "entity_end_2_in = []\n",
    "\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    \n",
    "    temp = data.iloc[i, :]\n",
    "    \n",
    "    s1 = temp['entity_start_1']\n",
    "    e1 = temp['entity_end_1']\n",
    "    \n",
    "    s2 = temp['entity_start_2']\n",
    "    e2 = temp['entity_end_2']\n",
    "    \n",
    "    if s1<=s2:\n",
    "        entity_start_1_in.extend([0])\n",
    "        entity_end_1_in.extend([e1 - s1])\n",
    "        \n",
    "        entity_start_2_in.extend([s2 - s1])\n",
    "        entity_end_2_in.extend([e2 - s1])\n",
    "        \n",
    "    else:\n",
    "        entity_start_2_in.extend([0])\n",
    "        entity_end_2_in.extend([e2 - s2])\n",
    "        \n",
    "        entity_start_1_in.extend([s1 - s2])\n",
    "        entity_end_1_in.extend([e1 - s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['entity_start_1_in'] = entity_start_1_in\n",
    "data['entity_start_2_in'] = entity_start_2_in\n",
    "data['entity_end_1_in'] = entity_end_1_in\n",
    "data['entity_end_2_in'] = entity_end_2_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************0**************\n",
      "*************10000**************\n",
      "*************20000**************\n",
      "*************30000**************\n",
      "*************40000**************\n",
      "*************50000**************\n",
      "*************60000**************\n",
      "*************70000**************\n"
     ]
    }
   ],
   "source": [
    "max_len = 16\n",
    "\n",
    "entity_location_vector_1 = []\n",
    "entity_location_vector_2 = []\n",
    "for i in range(data.shape[0]):\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        print('*************%s**************'%i)\n",
    "    temp = data.iloc[i, :]\n",
    "    \n",
    "    s1 = temp['entity_start_1_in']\n",
    "    e1 = temp['entity_end_1_in']\n",
    "    \n",
    "    left = list(range(min(int(max_len/2), s1))) + [int(max_len/2)]*(s1 - int(max_len/2))\n",
    "    left = [-1*x for x in left]\n",
    "    left.reverse()\n",
    "    location_vector_1 = left + [0]*(e1 - s1) + \\\n",
    "    list(range(min(int(max_len/2), int(max_len) - e1))) + [int(max_len/2)]*(int(max_len/2) - e1)\n",
    "    \n",
    "    entity_location_vector_1.append(np.array(location_vector_1))\n",
    "    \n",
    "    s2 = temp['entity_start_2_in']\n",
    "    e2 = temp['entity_end_2_in']\n",
    "    \n",
    "    left = list(range(min(int(max_len/2), s2))) + [int(max_len/2)]*(s2 - int(max_len/2))\n",
    "    left = [-1*x for x in left]\n",
    "    left.reverse()\n",
    "    location_vector_2 = left + [0]*(e2 - s2) + \\\n",
    "    list(range(min(int(max_len/2), int(max_len) - e2))) + [int(max_len/2)]*(int(max_len/2) - e2)\n",
    "    \n",
    "    entity_location_vector_2.append(np.array(location_vector_2))\n",
    "    \n",
    "data['entity_location_vector_1'] = entity_location_vector_1\n",
    "data['entity_location_vector_2'] = entity_location_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[:16]\n",
    "\n",
    "data['entity_location_vector_1'] = data['entity_location_vector_1'].map(func)\n",
    "data['entity_location_vector_2'] = data['entity_location_vector_2'].map(func)\n",
    "data['tag_all_seq'] = data['tag_all_seq'].map(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_mask = [[1, 1] if x != 'Other' else [0,1] for x in data['relation'] ]\n",
    "y_mask = np.array(y_mask,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define input function\n",
    "def get_input(X):\n",
    "    \n",
    "    input_ = [np.array(X.char_seq.values.tolist()), \\\n",
    "             np.array(X.entity_location_vector_1.values.tolist()), \\\n",
    "             np.array(X.entity_location_vector_2.values.tolist()), \\\n",
    "            np.array(X.tag_all_seq.values.tolist())]\n",
    "    \n",
    "    return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ranking_loss(y_true, y_pred):\n",
    "    y_true_one_hot = y_true[:, :6]  # [batch_size * 10], one-hot, all zero for other\n",
    "\n",
    "    y_label = y_true[:, 6:7]\n",
    "    y_label = tf.cast(y_label, tf.int32)\n",
    "    y_label = tf.reshape(y_label, shape=[-1])  # [ batch_size ], not one-hot label, a big number for other\n",
    "\n",
    "    y_mask = y_true[:, 7:]  # [batch_size * 2], [1, 1] for positive and [0, 1] for other\n",
    "\n",
    "    m_pos = 2.5\n",
    "    m_neg = 0.5\n",
    "    gamma = 2\n",
    "\n",
    "    pos_score = tf.multiply(y_true_one_hot, y_pred)\n",
    "    pos_score = tf.reduce_sum(pos_score, axis=1)\n",
    "\n",
    "    pos_loss = tf.exp(gamma * (m_pos - pos_score))\n",
    "    pos_loss = tf.log(1 + pos_loss)\n",
    "\n",
    "    pred_top_2_val = tf.nn.top_k(y_pred, k=2).values\n",
    "    pred_max_pos = tf.argmax(y_pred, axis=1)\n",
    "    pred_max_pos = tf.cast(pred_max_pos, tf.int32)\n",
    "    pred_is_max = tf.equal(pred_max_pos, y_label)\n",
    "    pred_is_max = tf.cast(pred_is_max, tf.float32)\n",
    "    pred_is_max = tf.expand_dims(pred_is_max, axis=-1)\n",
    "    inv_pred_is_max = 1 - pred_is_max\n",
    "    neg_score_mask = tf.concat([inv_pred_is_max, pred_is_max], axis=-1)\n",
    "    neg_score = tf.multiply(pred_top_2_val, neg_score_mask)\n",
    "    neg_score = tf.reduce_sum(neg_score, axis=1)\n",
    "    neg_loss = tf.exp(gamma * (m_neg + neg_score))\n",
    "    neg_loss = tf.log(1 + neg_loss)\n",
    "\n",
    "    pos_loss = tf.expand_dims(pos_loss, axis=-1)\n",
    "    neg_loss = tf.expand_dims(neg_loss, axis=-1)\n",
    "    ranking_loss = tf.concat([pos_loss, neg_loss], axis=-1)\n",
    "    ranking_loss = tf.multiply(y_mask, ranking_loss)\n",
    "    ranking_loss = tf.reduce_sum(ranking_loss, axis=1)\n",
    "    #     loss = tf.reduce_mean(ranking_loss)\n",
    "    #     return loss\n",
    "    return ranking_loss\n",
    "\n",
    "def MTL_model(cnn_filter_num = 64):\n",
    "    \n",
    "    #Input layer\n",
    "    \n",
    "    char_input = Input(shape=(16,), dtype='int32', name = 'input10')\n",
    "    entity_1_loc_input = Input(shape=(16,), dtype='int32', name = 'input11')\n",
    "    entity_2_loc_input = Input(shape=(16,), dtype='int32', name = 'input12')\n",
    "    tag_input = Input(shape=(16,), dtype='int32', name = 'input15')\n",
    "    \n",
    "    #embedding layer\n",
    "    char_embedding = Embedding(len(tokenizer_api.word_index)+1, 200,\n",
    "        input_length=16,#weights=[char_embedding_matrix],\n",
    "        trainable=True)\n",
    "    location_embedding = Embedding(150+1, 50, input_length = 16, \n",
    "                trainable=True, name = 'location_embedding')\n",
    "    tag_embedding = Embedding(133, 50, input_length = 16, \n",
    "                trainable=True, name = 'tag_embedding')\n",
    "    \n",
    "    \n",
    "    emb_char = char_embedding(char_input)\n",
    "    emb_entity_1_loc = location_embedding(entity_1_loc_input)\n",
    "    emb_entity_2_loc = location_embedding(entity_2_loc_input)\n",
    "    emb_tag = tag_embedding(tag_input)\n",
    "    \n",
    "    \n",
    "    emb_char = SpatialDropout1D(0.2)(emb_char)\n",
    "    emb_entity_1_loc = SpatialDropout1D(0.2)(emb_entity_1_loc)\n",
    "    emb_entity_2_loc = SpatialDropout1D(0.2)(emb_entity_2_loc)\n",
    "    emb_tag = SpatialDropout1D(0.2)(emb_tag)\n",
    "    \n",
    "    merge_embedding = concatenate([emb_char, emb_entity_1_loc, \\\n",
    "                                  emb_entity_2_loc, emb_tag])\n",
    "    \n",
    "    #multi size CNN for emb_char\n",
    "    \n",
    "    kernel_sizes = [1, 2, 3, ]\n",
    "    pooled_char = []\n",
    "    pooled_char_mean = []\n",
    "    \n",
    "    for kernel in kernel_sizes:\n",
    "\n",
    "        conv_char = Conv1D(filters=cnn_filter_num,\n",
    "                      kernel_size=kernel,\n",
    "                      padding='same',\n",
    "                      strides=1,\n",
    "                      kernel_initializer='he_uniform',\n",
    "                      activation='relu')(merge_embedding)\n",
    "        \n",
    "        pool_char = MaxPooling1D(pool_size = 16)(conv_char)\n",
    "        pool_char_2 = AvgPool1D(pool_size = 16)(conv_char)\n",
    "        \n",
    "        pooled_char.append(pool_char)\n",
    "        pooled_char_mean.append(pool_char_2)\n",
    "        \n",
    "    merged_pooled_char = Concatenate(axis=-1)(pooled_char)\n",
    "    flatten_pooled_char = Flatten()(merged_pooled_char)\n",
    "    \n",
    "    merged_pooled_char2 = Concatenate(axis=-1)(pooled_char_mean)\n",
    "    flatten_pooled_char2 = Flatten()(merged_pooled_char2)\n",
    "    \n",
    "    merge_all = concatenate([flatten_pooled_char, flatten_pooled_char2])#rnn_output\n",
    "    \n",
    "    merge_all = BatchNormalization()(merge_all)\n",
    "    merge_all = Dropout(0.5)(merge_all)\n",
    "    merge_all = Dense(128, activation='relu')(merge_all)\n",
    "    \n",
    "    merge_all = BatchNormalization()(merge_all)\n",
    "    merge_all = Dropout(0.2)(merge_all)\n",
    "    \n",
    "    pred1 = Dense(6, name = 'loss_1')(merge_all)\n",
    "    \n",
    "    pred2 = Dense(2, activation = 'softmax', name = 'loss_2')(merge_all)\n",
    "    \n",
    "    model = Model(inputs=[char_input, entity_1_loc_input, entity_2_loc_input,\n",
    "    tag_input],outputs=[pred1, pred2])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_binary = ['relation' if x != 'Other' else x for x in data['relation']]\n",
    "\n",
    "LB_y_binary = LabelEncoder()\n",
    "LB_y_binary.fit(y_binary)\n",
    "y_binary = LB_y_binary.transform(y_binary)\n",
    "\n",
    "y_binary = to_categorical(y_binary, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_id_list = list(data['docs_id'].unique())\n",
    "\n",
    "docs_id_list = list(data['docs_id'].unique())\n",
    "\n",
    "folds = [docs_id_list[:107], docs_id_list[107:214], docs_id_list[214:321], docs_id_list[321:428], docs_id_list[428:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_train = np.array(data[~pd.DataFrame(data['docs_id'])['docs_id'].isin(docs_id_list[:107])].index)\n",
    "idx_val = np.array(data[pd.DataFrame(data['docs_id'])['docs_id'].isin(docs_id_list[:107])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train n times to reduce the randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******0*******\n",
      "Train on 58699 samples, validate on 13196 samples\n",
      "Epoch 1/50\n",
      "58699/58699 [==============================] - 6s 95us/step - loss: 2.2896 - loss_1_loss: 1.9256 - loss_2_loss: 0.3640 - val_loss: 0.8250 - val_loss_1_loss: 0.6935 - val_loss_2_loss: 0.1314\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.82495, saving model to 0_bestmodel.hdf5\n",
      "Epoch 2/50\n",
      "58699/58699 [==============================] - 5s 78us/step - loss: 0.8729 - loss_1_loss: 0.7250 - loss_2_loss: 0.1479 - val_loss: 0.6762 - val_loss_1_loss: 0.5664 - val_loss_2_loss: 0.1098\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.82495 to 0.67618, saving model to 0_bestmodel.hdf5\n",
      "Epoch 3/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.7369 - loss_1_loss: 0.6151 - loss_2_loss: 0.1218 - val_loss: 0.6562 - val_loss_1_loss: 0.5424 - val_loss_2_loss: 0.1139\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67618 to 0.65625, saving model to 0_bestmodel.hdf5\n",
      "Epoch 4/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.6495 - loss_1_loss: 0.5403 - loss_2_loss: 0.1092 - val_loss: 0.5956 - val_loss_1_loss: 0.4859 - val_loss_2_loss: 0.1097\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.65625 to 0.59561, saving model to 0_bestmodel.hdf5\n",
      "Epoch 5/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.5749 - loss_1_loss: 0.4769 - loss_2_loss: 0.0980 - val_loss: 0.6106 - val_loss_1_loss: 0.4955 - val_loss_2_loss: 0.1151\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.59561\n",
      "Epoch 6/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.5178 - loss_1_loss: 0.4277 - loss_2_loss: 0.0902 - val_loss: 0.5864 - val_loss_1_loss: 0.4709 - val_loss_2_loss: 0.1154\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.59561 to 0.58638, saving model to 0_bestmodel.hdf5\n",
      "Epoch 7/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.4607 - loss_1_loss: 0.3781 - loss_2_loss: 0.0826 - val_loss: 0.6108 - val_loss_1_loss: 0.4921 - val_loss_2_loss: 0.1188\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58638\n",
      "Epoch 8/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.4142 - loss_1_loss: 0.3389 - loss_2_loss: 0.0752 - val_loss: 0.6101 - val_loss_1_loss: 0.4827 - val_loss_2_loss: 0.1274\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58638\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    GEN-AFF     0.6220    0.6475    0.6345       122\n",
      " PART-WHOLE     0.7722    0.7277    0.7493       191\n",
      "    ORG-AFF     0.8078    0.8058    0.8068       412\n",
      "       PHYS     0.6681    0.5889    0.6260       270\n",
      "        ART     0.5072    0.5344    0.5204       131\n",
      "    PER-SOC     0.6439    0.7025    0.6719       121\n",
      "\n",
      "avg / total     0.7064    0.6929    0.6988      1247\n",
      "\n",
      "*******1*******\n",
      "Train on 58699 samples, validate on 13196 samples\n",
      "Epoch 1/50\n",
      "58699/58699 [==============================] - 6s 97us/step - loss: 2.2731 - loss_1_loss: 1.9169 - loss_2_loss: 0.3562 - val_loss: 0.8335 - val_loss_1_loss: 0.7055 - val_loss_2_loss: 0.1280\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.83348, saving model to 0_bestmodel.hdf5\n",
      "Epoch 2/50\n",
      "58699/58699 [==============================] - 5s 78us/step - loss: 0.8704 - loss_1_loss: 0.7246 - loss_2_loss: 0.1458 - val_loss: 0.6891 - val_loss_1_loss: 0.5697 - val_loss_2_loss: 0.1194\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.83348 to 0.68909, saving model to 0_bestmodel.hdf5\n",
      "Epoch 3/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.7314 - loss_1_loss: 0.6087 - loss_2_loss: 0.1227 - val_loss: 0.6100 - val_loss_1_loss: 0.5004 - val_loss_2_loss: 0.1096\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68909 to 0.60999, saving model to 0_bestmodel.hdf5\n",
      "Epoch 4/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.6425 - loss_1_loss: 0.5347 - loss_2_loss: 0.1078 - val_loss: 0.6068 - val_loss_1_loss: 0.4948 - val_loss_2_loss: 0.1120\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60999 to 0.60682, saving model to 0_bestmodel.hdf5\n",
      "Epoch 5/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.5641 - loss_1_loss: 0.4682 - loss_2_loss: 0.0959 - val_loss: 0.5686 - val_loss_1_loss: 0.4568 - val_loss_2_loss: 0.1118\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.60682 to 0.56861, saving model to 0_bestmodel.hdf5\n",
      "Epoch 6/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.5089 - loss_1_loss: 0.4190 - loss_2_loss: 0.0899 - val_loss: 0.5766 - val_loss_1_loss: 0.4630 - val_loss_2_loss: 0.1135\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.56861\n",
      "Epoch 7/50\n",
      "58699/58699 [==============================] - 5s 77us/step - loss: 0.4568 - loss_1_loss: 0.3747 - loss_2_loss: 0.0821 - val_loss: 0.6153 - val_loss_1_loss: 0.4935 - val_loss_2_loss: 0.1218\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.56861\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    GEN-AFF     0.7654    0.5082    0.6108       122\n",
      " PART-WHOLE     0.7933    0.7435    0.7676       191\n",
      "    ORG-AFF     0.8376    0.7888    0.8125       412\n",
      "       PHYS     0.6080    0.6778    0.6410       270\n",
      "        ART     0.6344    0.4504    0.5268       131\n",
      "    PER-SOC     0.7059    0.6942    0.7000       121\n",
      "\n",
      "avg / total     0.7399    0.6856    0.7078      1247\n",
      "\n",
      "*******2*******\n",
      "Train on 58699 samples, validate on 13196 samples\n",
      "Epoch 1/50\n",
      "58699/58699 [==============================] - 6s 97us/step - loss: 2.3223 - loss_1_loss: 1.9677 - loss_2_loss: 0.3546 - val_loss: 0.8260 - val_loss_1_loss: 0.6905 - val_loss_2_loss: 0.1355\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.82604, saving model to 0_bestmodel.hdf5\n",
      "Epoch 2/50\n",
      "58699/58699 [==============================] - 5s 78us/step - loss: 0.8787 - loss_1_loss: 0.7322 - loss_2_loss: 0.1465 - val_loss: 0.6907 - val_loss_1_loss: 0.5784 - val_loss_2_loss: 0.1123\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.82604 to 0.69073, saving model to 0_bestmodel.hdf5\n",
      "Epoch 3/50\n",
      "58699/58699 [==============================] - 5s 85us/step - loss: 0.7379 - loss_1_loss: 0.6177 - loss_2_loss: 0.1202 - val_loss: 0.6231 - val_loss_1_loss: 0.5154 - val_loss_2_loss: 0.1077\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69073 to 0.62312, saving model to 0_bestmodel.hdf5\n",
      "Epoch 4/50\n",
      "58699/58699 [==============================] - 5s 84us/step - loss: 0.6520 - loss_1_loss: 0.5449 - loss_2_loss: 0.1070 - val_loss: 0.6122 - val_loss_1_loss: 0.4978 - val_loss_2_loss: 0.1144\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62312 to 0.61223, saving model to 0_bestmodel.hdf5\n",
      "Epoch 5/50\n",
      "58699/58699 [==============================] - 5s 81us/step - loss: 0.5744 - loss_1_loss: 0.4782 - loss_2_loss: 0.0962 - val_loss: 0.5888 - val_loss_1_loss: 0.4759 - val_loss_2_loss: 0.1129\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61223 to 0.58879, saving model to 0_bestmodel.hdf5\n",
      "Epoch 6/50\n",
      "58699/58699 [==============================] - 5s 87us/step - loss: 0.5125 - loss_1_loss: 0.4240 - loss_2_loss: 0.0885 - val_loss: 0.5864 - val_loss_1_loss: 0.4733 - val_loss_2_loss: 0.1131\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.58879 to 0.58638, saving model to 0_bestmodel.hdf5\n",
      "Epoch 7/50\n",
      "58699/58699 [==============================] - 5s 86us/step - loss: 0.4565 - loss_1_loss: 0.3779 - loss_2_loss: 0.0786 - val_loss: 0.5956 - val_loss_1_loss: 0.4779 - val_loss_2_loss: 0.1177\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58638\n",
      "Epoch 8/50\n",
      "58699/58699 [==============================] - 5s 79us/step - loss: 0.4187 - loss_1_loss: 0.3454 - loss_2_loss: 0.0733 - val_loss: 0.6262 - val_loss_1_loss: 0.4975 - val_loss_2_loss: 0.1287\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58638\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    GEN-AFF     0.7273    0.5246    0.6095       122\n",
      " PART-WHOLE     0.7811    0.6911    0.7333       191\n",
      "    ORG-AFF     0.7562    0.8131    0.7836       412\n",
      "       PHYS     0.6095    0.6185    0.6140       270\n",
      "        ART     0.5746    0.5878    0.5811       131\n",
      "    PER-SOC     0.6462    0.6942    0.6693       121\n",
      "\n",
      "avg / total     0.6957    0.6889    0.6898      1247\n",
      "\n",
      "*******3*******\n",
      "Train on 58699 samples, validate on 13196 samples\n",
      "Epoch 1/50\n",
      "58699/58699 [==============================] - 6s 106us/step - loss: 2.2941 - loss_1_loss: 1.9457 - loss_2_loss: 0.3484 - val_loss: 0.8732 - val_loss_1_loss: 0.7027 - val_loss_2_loss: 0.1705\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.87322, saving model to 0_bestmodel.hdf5\n",
      "Epoch 2/50\n",
      "58699/58699 [==============================] - 5s 86us/step - loss: 0.8738 - loss_1_loss: 0.7267 - loss_2_loss: 0.1472 - val_loss: 0.6910 - val_loss_1_loss: 0.5694 - val_loss_2_loss: 0.1215\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.87322 to 0.69099, saving model to 0_bestmodel.hdf5\n",
      "Epoch 3/50\n",
      "58699/58699 [==============================] - 5s 81us/step - loss: 0.7308 - loss_1_loss: 0.6093 - loss_2_loss: 0.1215 - val_loss: 0.6087 - val_loss_1_loss: 0.4995 - val_loss_2_loss: 0.1091\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69099 to 0.60866, saving model to 0_bestmodel.hdf5\n",
      "Epoch 4/50\n",
      "58699/58699 [==============================] - 5s 91us/step - loss: 0.6421 - loss_1_loss: 0.5341 - loss_2_loss: 0.1079 - val_loss: 0.5954 - val_loss_1_loss: 0.4803 - val_loss_2_loss: 0.1151\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60866 to 0.59536, saving model to 0_bestmodel.hdf5\n",
      "Epoch 5/50\n",
      "58699/58699 [==============================] - 5s 86us/step - loss: 0.5765 - loss_1_loss: 0.4782 - loss_2_loss: 0.0983 - val_loss: 0.5769 - val_loss_1_loss: 0.4641 - val_loss_2_loss: 0.1128\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.59536 to 0.57691, saving model to 0_bestmodel.hdf5\n",
      "Epoch 6/50\n",
      "58699/58699 [==============================] - 5s 84us/step - loss: 0.5063 - loss_1_loss: 0.4175 - loss_2_loss: 0.0888 - val_loss: 0.5907 - val_loss_1_loss: 0.4742 - val_loss_2_loss: 0.1166\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.57691\n",
      "Epoch 7/50\n",
      "58699/58699 [==============================] - 5s 79us/step - loss: 0.4517 - loss_1_loss: 0.3713 - loss_2_loss: 0.0803 - val_loss: 0.5848 - val_loss_1_loss: 0.4677 - val_loss_2_loss: 0.1171\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    GEN-AFF     0.7010    0.5574    0.6210       122\n",
      " PART-WHOLE     0.7713    0.7592    0.7652       191\n",
      "    ORG-AFF     0.8705    0.7670    0.8155       412\n",
      "       PHYS     0.6129    0.6333    0.6230       270\n",
      "        ART     0.6235    0.4046    0.4907       131\n",
      "    PER-SOC     0.7083    0.7025    0.7054       121\n",
      "\n",
      "avg / total     0.7413    0.6720    0.7023      1247\n",
      "\n",
      "*******4*******\n",
      "Train on 58699 samples, validate on 13196 samples\n",
      "Epoch 1/50\n",
      "58699/58699 [==============================] - 6s 102us/step - loss: 2.2703 - loss_1_loss: 1.9157 - loss_2_loss: 0.3546 - val_loss: 0.8369 - val_loss_1_loss: 0.6941 - val_loss_2_loss: 0.1428\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.83695, saving model to 0_bestmodel.hdf5\n",
      "Epoch 2/50\n",
      "58699/58699 [==============================] - 5s 79us/step - loss: 0.8738 - loss_1_loss: 0.7275 - loss_2_loss: 0.1463 - val_loss: 0.6918 - val_loss_1_loss: 0.5728 - val_loss_2_loss: 0.1190\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.83695 to 0.69176, saving model to 0_bestmodel.hdf5\n",
      "Epoch 3/50\n",
      "58699/58699 [==============================] - 5s 79us/step - loss: 0.7361 - loss_1_loss: 0.6135 - loss_2_loss: 0.1225 - val_loss: 0.6314 - val_loss_1_loss: 0.5180 - val_loss_2_loss: 0.1134\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69176 to 0.63137, saving model to 0_bestmodel.hdf5\n",
      "Epoch 4/50\n",
      "58699/58699 [==============================] - 5s 85us/step - loss: 0.6505 - loss_1_loss: 0.5419 - loss_2_loss: 0.1086 - val_loss: 0.5820 - val_loss_1_loss: 0.4718 - val_loss_2_loss: 0.1102\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63137 to 0.58199, saving model to 0_bestmodel.hdf5\n",
      "Epoch 5/50\n",
      "58699/58699 [==============================] - 5s 79us/step - loss: 0.5715 - loss_1_loss: 0.4748 - loss_2_loss: 0.0967 - val_loss: 0.5811 - val_loss_1_loss: 0.4695 - val_loss_2_loss: 0.1116\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58199 to 0.58106, saving model to 0_bestmodel.hdf5\n",
      "Epoch 6/50\n",
      "58699/58699 [==============================] - 5s 81us/step - loss: 0.5061 - loss_1_loss: 0.4179 - loss_2_loss: 0.0882 - val_loss: 0.5817 - val_loss_1_loss: 0.4681 - val_loss_2_loss: 0.1136\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.58106\n",
      "Epoch 7/50\n",
      "58699/58699 [==============================] - 5s 85us/step - loss: 0.4559 - loss_1_loss: 0.3747 - loss_2_loss: 0.0812 - val_loss: 0.6295 - val_loss_1_loss: 0.5078 - val_loss_2_loss: 0.1217\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.58106\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    GEN-AFF     0.7111    0.5246    0.6038       122\n",
      " PART-WHOLE     0.7037    0.7958    0.7469       191\n",
      "    ORG-AFF     0.7791    0.8131    0.7957       412\n",
      "       PHYS     0.6818    0.5000    0.5769       270\n",
      "        ART     0.6515    0.3282    0.4365       131\n",
      "    PER-SOC     0.6641    0.7025    0.6827       121\n",
      "\n",
      "avg / total     0.7153    0.6528    0.6734      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    print('*******%s*******'%n)\n",
    "\n",
    "    X_train = get_input(data.loc[idx_train, :])\n",
    "    y_train = y_binary[idx_train]\n",
    "\n",
    "    X_val = get_input(data.loc[idx_val, :])\n",
    "    y_val = y_binary[idx_val]\n",
    "    y_train_mask = y_mask[idx_train]\n",
    "    y_val_mask = y_mask[idx_val]\n",
    "    \"\"\"\n",
    "    Process y\n",
    "    \"\"\"\n",
    "    y_train_6 =  np.delete(y[idx_train], 3, axis = 1)\n",
    "    y_train_label_6 = []\n",
    "    for x in y_train_6:\n",
    "        if np.max(x) == 0:\n",
    "            y_train_label_6 .append(8888)\n",
    "        else:\n",
    "            y_train_label_6 .append(np.argmax(x))\n",
    "    y_train_label_6 = np.array(y_train_label_6)\n",
    "    y_train_label_6 = np.expand_dims(y_train_label_6, axis=-1)\n",
    "    y_train_ranking = np.concatenate([y_train_6, y_train_label_6, y_train_mask], axis=1)\n",
    "\n",
    "    y_val_6 =  np.delete(y[idx_val], 3, axis = 1)\n",
    "    y_val_label_6 = []\n",
    "    for x in y_val_6:\n",
    "        if np.max(x) == 0:\n",
    "            y_val_label_6 .append(8888)\n",
    "        else:\n",
    "            y_val_label_6 .append(np.argmax(x))\n",
    "    y_val_label_6 = np.array(y_val_label_6)\n",
    "    y_val_label_6 = np.expand_dims(y_val_label_6, axis=-1)\n",
    "    y_val_ranking = np.concatenate([y_val_6, y_val_label_6, y_val_mask], axis=1)\n",
    "\n",
    "    bst_model_path =  str(i) + '_bestmodel.hdf5'\n",
    "\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "\n",
    "    model = MTL_model()\n",
    "    model.compile(loss={'loss_1':ranking_loss, 'loss_2':'categorical_crossentropy'},\n",
    "        optimizer='RMSprop',\n",
    "        loss_weights={'loss_1':1, 'loss_2':1})\n",
    "        #metrics=[ranking_loss])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, mode = 'min')\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, monitor = 'val_loss', \n",
    "         save_best_only=True, save_weights_only=True, verbose=1,  mode = 'min')\n",
    "    callbacks = [\n",
    "                early_stopping,\n",
    "                model_checkpoint\n",
    "            ]\n",
    "\n",
    "    hist = model.fit(X_train, {'loss_1':y_train_ranking, 'loss_2':y_train},verbose=True,\\\n",
    "        validation_data=(X_val, {'loss_1':y_val_ranking, 'loss_2':y_val}), \\\n",
    "        epochs=50, batch_size=256, shuffle=True, callbacks=callbacks)#callbacks=callbacks, \n",
    "#             del model\n",
    "\n",
    "    #  test for f1\n",
    "#     p1 = model.predict(X_val)\n",
    "#     p1 = pd.DataFrame(p1)\n",
    "\n",
    "#     result = pd.DataFrame(LB_y.inverse_transform(np.argmax(np.array(p1),axis = 1)))\n",
    "#     true = pd.DataFrame(LB_y.inverse_transform(np.argmax(np.array(y_val),axis = 1)))\n",
    "\n",
    "    label = list(data['relation'].unique())\n",
    "    label.remove('Other')\n",
    "\n",
    "#         model = MTL_model()\n",
    "#         model.load_weights(bst_model_path)\n",
    "\n",
    "    model.load_weights(bst_model_path)\n",
    "\n",
    "    p1 = model.predict(X_val)[0]\n",
    "\n",
    "    true_p1 = np.insert(np.array(p1), 3, values=0.5, axis=1)\n",
    "    true_result = []\n",
    "    for x in true_p1:\n",
    "        max_score = np.max(x)\n",
    "        max_arg = np.argmax(x)\n",
    "        true_result.append(max_arg)\n",
    "    true_result =np.array(true_result)\n",
    "\n",
    "    true_true =np.array(np.argmax(y[idx_val], axis = 1))\n",
    "\n",
    "\n",
    "    result = pd.DataFrame(LB_y.inverse_transform(true_result))\n",
    "    true = pd.DataFrame(LB_y.inverse_transform(true_true))\n",
    "\n",
    "    f1_result = classification_report(list(true[0]),list(result[0]), labels = label, digits = 4)\n",
    "    print(f1_result)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
